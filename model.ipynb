{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor, DecisionTreeRegressor, GeneralizedLinearRegression, IsotonicRegression, AFTSurvivalRegression, FMRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import numpy as np\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fee2018364fbc9c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c63d38f13a9f2e6b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder.master('local').appName(\"HousePricePrediction\").getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "459f210ec4366725"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data from CSV file\n",
    "data = spark.read.csv(\"housing.csv\", header=True, inferSchema=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df7c47b434cff8b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 3: Analyze the data\n",
    "data.printSchema()\n",
    "data.show(5)\n",
    "print(\"Total number of rows:\", data.count())\n",
    "\n",
    "for col in [\"ocean_proximity\"]:\n",
    "    print(f\"Unique values in {col}:\")\n",
    "    data.select(col).distinct().show()\n",
    "\n",
    "for col in data.columns:\n",
    "    print(f\"NA rate in {col}: {data.filter(data[col].isNull()).count() / data.count()}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f272c2358ab93dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Do one-hot encoding for categorical columns\n",
    "indexer = StringIndexer(inputCol=\"ocean_proximity\", outputCol=\"ocean_proximity_index\")\n",
    "data = indexer.fit(data).transform(data)\n",
    "encoder = OneHotEncoder(inputCol=\"ocean_proximity_index\", outputCol=\"ocean_proximity_encoded\")\n",
    "encModel = encoder.fit(data)\n",
    "data = encModel.transform(data)\n",
    "\n",
    "\n",
    "data.show(5)\n",
    "print(\"Total number of rows:\", data.count())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67d0039a3219071e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Handle missing values in total_bedrooms column\n",
    "data = data.fillna(data.approxQuantile(\"total_bedrooms\", [0.5], 0.001)[0], subset=[\"total_bedrooms\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0c55cbdb24a4887"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def rfe_feature_selection(data, features, target_col, num_features, estimator):\n",
    "    # Initialize remaining features and selected features\n",
    "    remaining_features = features\n",
    "    selected_features = []\n",
    "\n",
    "    while len(selected_features) < num_features:\n",
    "        # Create VectorAssembler for remaining features\n",
    "        assembler = VectorAssembler(inputCols=remaining_features, outputCol=\"features\")\n",
    "        data_assembled = assembler.transform(data)\n",
    "\n",
    "        # Train Random Forest model\n",
    "        rf = estimator(labelCol=target_col, featuresCol=\"features\")\n",
    "        model = rf.fit(data_assembled)\n",
    "\n",
    "        # Get feature importances\n",
    "        importances = model.featureImportances\n",
    "\n",
    "        # Convert SparseVector to dense representation\n",
    "        importances_dense = importances.toArray()\n",
    "\n",
    "\n",
    "        # Find the least important feature\n",
    "        least_important_feature_index = np.argmin(importances_dense)\n",
    "\n",
    "        least_important_feature = remaining_features[least_important_feature_index]\n",
    "\n",
    "\n",
    "        # Remove least important feature from remaining features\n",
    "        remaining_features.remove(least_important_feature)\n",
    "\n",
    "        # Add least important feature to selected features\n",
    "        selected_features.append(least_important_feature)\n",
    "\n",
    "    # Select final features\n",
    "    return data.select(selected_features + [target_col])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5f6c92a548dab49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select reamining features except target column\n",
    "features = [col for col in data.columns if col != \"ocean_proximity_encoded\" and col != \"ocean_proximity\"]\n",
    "\n",
    "# Eleminate not necassary features\n",
    "num_features = 8  # Smaller Values giving very bad results\n",
    "data_filtered = rfe_feature_selection(data, features, \"median_house_value\", num_features, RandomForestRegressor)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "403590c40483419d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(data_filtered.columns)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb5da17a76a6f978"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select reamining features except target column\n",
    "features = [col for col in data_filtered.columns if col != \"median_house_value\"]\n",
    "if \"ocean_proximity_index\" in features:\n",
    "    features.remove(\"ocean_proximity_index\")\n",
    "    features.append(\"ocean_proximity_encoded\")\n",
    "\n",
    "# Create a VectorAssembler to combine features into a single vector\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "data = scaler.fit(data).transform(data)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2af760b88b411e9a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a Regression models\n",
    "models = [\n",
    "    (\"Linear Regression\", LinearRegression(labelCol=\"median_house_value\", featuresCol=\"scaledFeatures\")),\n",
    "    (\"Random Forest\", RandomForestRegressor(labelCol=\"median_house_value\", featuresCol=\"scaledFeatures\")),\n",
    "    (\"Gradient-Boosted Tree\", GBTRegressor(labelCol=\"median_house_value\", featuresCol=\"scaledFeatures\")),\n",
    "    (\"Decision Tree\", DecisionTreeRegressor(labelCol=\"median_house_value\", featuresCol=\"scaledFeatures\")),\n",
    "    (\"Generalized Linear Regression\", GeneralizedLinearRegression(labelCol=\"median_house_value\", featuresCol=\"scaledFeatures\")),\n",
    "    (\"Factorization Machines\", FMRegressor(labelCol=\"median_house_value\", featuresCol=\"scaledFeatures\"))\n",
    "]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b32f0ef50f9d1da8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"median_house_value\", predictionCol=\"prediction\")\n",
    "\n",
    "for name, model in models:\n",
    "    model = model.fit(train_data)\n",
    "    predictions = model.transform(test_data)\n",
    "    mse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n",
    "    rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "    r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"    MSE: {mse}, RMSE: {rmse}, R-squared: {r2}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7117d7c4b0b274a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the best model\n",
    "best_model = GBTRegressor(labelCol=\"median_house_value\", featuresCol=\"scaledFeatures\")\n",
    "best_model = best_model.fit(train_data)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b10e96a45339d7dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate best model\n",
    "evaluator = RegressionEvaluator(labelCol=\"median_house_value\", predictionCol=\"prediction\")\n",
    "predictions = best_model.transform(test_data)\n",
    "mse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "explained_variance = evaluator.evaluate(predictions, {evaluator.metricName: \"var\"})\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"Best Model: Gradient-Boosted Tree\")\n",
    "print(f\"    MSE: {mse}, RMSE: {rmse}, MAE: {mae}, R-squared: {r2}, Explained Variance: {explained_variance}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d317d48ed60768b4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the best model on the entire dataset\n",
    "best_model = GBTRegressor(labelCol=\"median_house_value\", featuresCol=\"scaledFeatures\")\n",
    "best_model = best_model.fit(data)\n",
    "\n",
    "# Save the model\n",
    "best_model.save(\"best_model\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e236771d30686fd3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Zip the model\n",
    "import shutil\n",
    "shutil.make_archive(\"best_model\", 'zip', \"best_model\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a5d2234af7cc270"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Stop SparkSession\n",
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe866e0f23bbf300"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b3d1f56bfaa860f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
